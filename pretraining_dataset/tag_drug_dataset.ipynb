{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5hLSghmtqMxCmc6DFYeD0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahopman/IEBM-Net/blob/main/pretraining_dataset/tag_drug_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = 'Qiao Jin'\n",
        "__editor__ = 'Mia Hopman'"
      ],
      "metadata": {
        "id": "-rN8PZC_u2zk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_path = '/content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data'\n",
        "pretraining_dataset_path = f'{local_path}/pretraining_dataset'"
      ],
      "metadata": {
        "id": "FQYh1mTWu50a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tg6w1-RbuyK8",
        "outputId": "dcc4c8b4-86ab-49dd-a934-75b14005ea66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-05 18:27:53--  http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip [following]\n",
            "--2024-05-05 18:27:54--  https://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-postagger-full-2015-04-20.zip [following]\n",
            "--2024-05-05 18:27:54--  https://downloads.cs.stanford.edu/nlp/software/stanford-postagger-full-2015-04-20.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 129905428 (124M) [application/zip]\n",
            "Saving to: ‘/content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20.zip’\n",
            "\n",
            "stanford-postagger- 100%[===================>] 123.89M  5.12MB/s    in 20s     \n",
            "\n",
            "2024-05-05 18:28:14 (6.14 MB/s) - ‘/content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20.zip’ saved [129905428/129905428]\n",
            "\n",
            "Archive:  /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20.zip\n",
            "   creating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/\n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/README.txt  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/sample-input.txt  \n",
            "   creating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/data/\n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/data/enclitic-inflections.data  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/build.xml  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/stanford-postagger.sh  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/stanford-postagger-3.5.2.jar  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/stanford-postagger-gui.sh  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/stanford-postagger-3.5.2-javadoc.jar  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/stanford-postagger.jar  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/stanford-postagger.bat  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/sample-output.txt  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/stanford-postagger-3.5.2-sources.jar  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/TaggerDemo2.java  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/stanford-postagger-gui.bat  \n",
            "   creating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/\n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/german-fast-caseless.tagger  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/german-dewac.tagger  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/arabic-train.tagger.props  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/chinese-distsim.tagger  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/wsj-0-18-bidirectional-distsim.tagger  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/chinese-nodistsim.tagger.props  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/english-bidirectional-distsim.tagger.props  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/wsj-0-18-bidirectional-nodistsim.tagger  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/wsj-0-18-caseless-left3words-distsim.tagger  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/chinese-nodistsim.tagger  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/spanish.tagger  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/german-fast-caseless.tagger.props  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/README-Models.txt  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/english-caseless-left3words-distsim.tagger  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/french.tagger.props  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/wsj-0-18-left3words-distsim.tagger  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/english-bidirectional-distsim.tagger  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/spanish-distsim.tagger.props  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/english-left3words-distsim.tagger  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/wsj-0-18-left3words-nodistsim.tagger  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/wsj-0-18-left3words-nodistsim.tagger.props  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/wsj-0-18-left3words-distsim.tagger.props  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/english-caseless-left3words-distsim.tagger.props  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/arabic.tagger  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/arabic.tagger.props  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/german-fast.tagger  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/wsj-0-18-caseless-left3words-distsim.tagger.props  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/wsj-0-18-bidirectional-distsim.tagger.props  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/spanish-distsim.tagger  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/chinese-distsim.tagger.props  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/wsj-0-18-bidirectional-nodistsim.tagger.props  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/german-fast.tagger.props  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/english-left3words-distsim.tagger.props  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/french.tagger  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/german-hgc.tagger.props  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/german-dewac.tagger.props  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/arabic-train.tagger  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/german-hgc.tagger  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/models/spanish.tagger.props  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/TaggerDemo.java  \n",
            "  inflating: /content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data/stanford_pos/stanford-postagger-full-2015-04-20/LICENSE.txt  \n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip -P {local_path}/stanford_pos\n",
        "!unzip {local_path}/stanford_pos/stanford-postagger-full-2015-04-20.zip -d {local_path}/stanford_pos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Alibaba-NLP/EBM-Net.git\n",
        "!cp /content/EBM-Net/pretraining_dataset/sec2label.json -d {pretraining_dataset_path}/sec2label.json"
      ],
      "metadata": {
        "id": "u2lLzit3-VAA",
        "outputId": "6afb97e5-0903-4f94-ad86-a51b4ded9271",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'EBM-Net' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_and_label(sent):\n",
        "\tsent = ' ' + sent\n",
        "\tlower_sent = sent.lower()\n",
        "\n",
        "\tif ' than ' in lower_sent and all([exc not in lower_sent for exc in exclude]):\n",
        "\t\twords = tokenize.word_tokenize(sent)\n",
        "\t\tlowers = [word.lower() for word in words]\n",
        "\t\twords_ctr = Counter(lowers)\n",
        "\n",
        "\t\tif words_ctr['than'] == 1: # more than 1 are not useful (mostly describing only quantitative relations)\n",
        "\t\t\tthan_idx = lowers.index('than')\n",
        "\t\t\tinter = set(lowers[:than_idx]).intersection(key_words)\n",
        "\n",
        "\t\t\tif len(inter) >= 1:\n",
        "\t\t\t\tup_indices = [1 if word.lower() in ups else 0 for word in words]\n",
        "\t\t\t\tdown_indices = [1 if word.lower() in downs else 0 for word in words]\n",
        "\n",
        "\t\t\t\tif any(up_indices) and not any(down_indices):\n",
        "\t\t\t\t\tif than_idx + 1 < len(lowers) and (lowers[than_idx+1].isnumeric() or lowers[than_idx+1] in nums):\n",
        "\t\t\t\t\t\tpass\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tindices = [idx_ for idx_, up in enumerate(up_indices) if up == 1] + [than_idx]\n",
        "\t\t\t\t\t\tfinal = words\n",
        "\t\t\t\t\t\tdirection = 2\n",
        "\n",
        "\t\t\t\telif any(down_indices) and not any(up_indices):\n",
        "\t\t\t\t\tif than_idx + 1 < len(lowers) and (lowers[than_idx+1].isnumeric() or lowers[than_idx+1] in nums):\n",
        "\t\t\t\t\t\tpass\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tindices = [idx_ for idx_, down in enumerate(down_indices) if down == 1] + [than_idx]\n",
        "\t\t\t\t\t\tfinal = words\n",
        "\t\t\t\t\t\tdirection = 0\n",
        "\n",
        "\telif ' similar' in lower_sent and ' to ' in lower_sent:\n",
        "\t\twords = tokenize.word_tokenize(sent)\n",
        "\t\tlowers = [word.lower() for word in words]\n",
        "\t\twords_ctr = Counter(lowers)\n",
        "\n",
        "\t\tfor idx, lower in enumerate(lowers):\n",
        "\t\t\tif lower in sims:\n",
        "\t\t\t\tsim_idx = idx\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\tif 'sim_idx' in locals():\n",
        "\t\t\tif 'to' in lowers[sim_idx:]:\n",
        "\t\t\t\tto_idx = sim_idx + lowers[sim_idx:].index('to')\n",
        "\n",
        "\t\t\t\tindices = [sim_idx] + [to_idx]\n",
        "\t\t\t\tfinal = words\n",
        "\t\t\t\tdirection = 1\n",
        "\n",
        "\telif ' no' in lower_sent and ' differ' in lower_sent and 'and' in lower_sent:\n",
        "\t\twords = tokenize.word_tokenize(sent)\n",
        "\t\tlowers = [word.lower() for word in words]\n",
        "\t\twords_ctr = Counter(lowers)\n",
        "\n",
        "\t\tfor idx, word in enumerate(lowers):\n",
        "\t\t\tif word in diffs:\n",
        "\t\t\t\tdiff_idx = idx\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\tif 'diff_idx' in locals():\n",
        "\t\t\t# first find the left no, then scan the middle words\n",
        "\t\t\tfor i in range(idx):\n",
        "\t\t\t\tword = words[idx-1-i]\n",
        "\t\t\t\tif word in nos:\n",
        "\t\t\t\t\tno_idx = idx-1-i\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\tif 'no_idx' in locals():\n",
        "\t\t\t\tbet_indices = [1 if word == 'between' and idx > diff_idx else 0 for idx, word in enumerate(lowers)]\n",
        "\n",
        "\t\t\t\tif any(bet_indices):\n",
        "\t\t\t\t\tfirst_bet = bet_indices.index(1)\n",
        "\t\t\t\t\tif 'and' in lowers[first_bet:]:\n",
        "\t\t\t\t\t\tand_idx = first_bet + lowers[first_bet:].index('and')\n",
        "\t\t\t\t\t\tindices = list(range(no_idx, diff_idx+1)) + [idx for idx, bet in enumerate(bet_indices) if bet == 1] + [and_idx]\n",
        "\t\t\t\t\t\tfinal = words\n",
        "\t\t\t\t\t\tdirection = 1\n",
        "\n",
        "\tif 'final' in locals() and 'direction' in locals():\n",
        "\t\tif type(final) == list and len(final) > 0 and final[-1] != '?':\n",
        "\t\t\treturn [final, direction, indices]\n",
        "\n",
        "\telse:\n",
        "\t\treturn False"
      ],
      "metadata": {
        "id": "OIv6XXtcvg4m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process(item):\n",
        "\t# an item is an article\n",
        "\t# also need to save the context\n",
        "\t# as well as save the evidence\n",
        "\tpmid = item['pmid']\n",
        "\ttexts = item['texts']\n",
        "\tlabels = item['sec_labels']\n",
        "\n",
        "\tevi_output = []\n",
        "\tctx_output = {'pmid': pmid, 'ctx': ''}\n",
        "\n",
        "\tbg_status = True\n",
        "\n",
        "\tfor text, label in zip(texts, labels):\n",
        "\t\tif label == 'TITLE': continue\n",
        "\t\tsents = tokenize.sent_tokenize(text)\n",
        "\t\tif not label or label not in sec2label:\n",
        "\t\t\tfor sent in sents:\n",
        "\t\t\t\tresult = mask_and_label(sent)\n",
        "\t\t\t\tif result:\n",
        "\t\t\t\t\tbg_status = False\n",
        "\t\t\t\t\tevi_output.append({'pmid': pmid, 'pos': result[0], 'label': result[1], 'indices': result[2]})\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tif bg_status:\n",
        "\t\t\t\t\t\tctx_output['ctx'] += ' ' + sent\n",
        "\t\telse:\n",
        "\t\t\tjudge = sec2label[label]\n",
        "\n",
        "\t\t\tif judge == '1': # all background\n",
        "\t\t\t\tctx_output['ctx'] += ' ' + text\n",
        "\t\t\telse:\n",
        "\t\t\t\tbg_status = False # starting no background\n",
        "\t\t\t\tfor sent in sents:\n",
        "\t\t\t\t\tresult = mask_and_label(sent)\n",
        "\t\t\t\t\tif result: evi_output.append({'pmid': pmid, 'pos': result[0], 'label': result[1], 'indices': result[2]})\n",
        "\n",
        "\treturn evi_output, ctx_output"
      ],
      "metadata": {
        "id": "BQl8NY3zvmiv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir {pretraining_dataset_path}/evidence"
      ],
      "metadata": {
        "id": "iD0gJoPlCJlw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "from nltk.tag import StanfordPOSTagger\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import sys\n",
        "\n",
        "import random as rd\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "'''\n",
        "used to pseudo label the dataset\n",
        "'''\n",
        "\n",
        "jar = f'{local_path}/stanford_pos/stanford-postagger-full-2015-04-20/stanford-postagger.jar'\n",
        "model = f'{local_path}/stanford_pos/stanford-postagger-full-2015-04-20/models/english-left3words-distsim.tagger'\n",
        "\n",
        "st = StanfordPOSTagger(model, jar, encoding='utf8')\n",
        "\n",
        "exclude = set(['rather than', 'other than'])\n",
        "ups = set(['better', 'greater', 'higher', 'later', 'more', 'faster', 'older', 'longer', \\\n",
        "\t\t'larger', 'broader', 'wider', 'stronger', 'deeper', 'more', 'commoner', 'richer', \\\n",
        "\t\t'further', 'bigger'])\n",
        "downs = set(['worse', 'smaller', 'lower', 'earlier', 'less', 'slower', 'younger', 'shorter', \\\n",
        "\t\t'smaller', 'narrower', 'narrower', 'weaker', 'shallower', 'fewer', 'rarer', 'poorer', \\\n",
        "\t\t'closer', 'smaller'])\n",
        "\n",
        "key_words = ups.union(downs)\n",
        "\n",
        "diffs = set(['difference', 'differences', 'different', 'differently', 'differ'])\n",
        "sims = set(['similar', 'similarly', 'similarity', 'similarities'])\n",
        "\n",
        "nos = set(['no', 'not'])\n",
        "middles = set(['significant', 'significantly', 'statistic', 'statistically', 'statistical'])\n",
        "\n",
        "nums = set([\"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\", \"zero\", \\\n",
        "            \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"eleven\", \\\n",
        "            \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\", \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\"])\n",
        "\n",
        "sec2label = json.load(open(f'{pretraining_dataset_path}/sec2label.json'))\n",
        "\n",
        "files = glob.glob(f'{pretraining_dataset_path}/pubmed_baseline/pubmed24n*.json')\n",
        "for file in files:\n",
        "    chunk_id = file.split('/')[-1].split('.')[0].split('n')[-1]\n",
        "    if not os.path.exists(f'{pretraining_dataset_path}/evidence/evidence_pos_{chunk_id}.json'):\n",
        "        evi_output = []\n",
        "        ctx_output = []\n",
        "\n",
        "        data = json.load(open(file))\n",
        "\n",
        "        for item in data:\n",
        "            results = process(item)\n",
        "            evi_output += results[0]\n",
        "            ctx_output.append(results[1])\n",
        "\n",
        "        pos_list = st.tag_sents(o['pos'] for o in evi_output)\n",
        "\n",
        "        for _idx in range(len(evi_output)):\n",
        "            evi_output[_idx]['pos'] = pos_list[_idx]\n",
        "\n",
        "        with open(f'{pretraining_dataset_path}/evidence/evidence_pos_{chunk_id}.json', 'w') as f:\n",
        "            json.dump(evi_output, f)\n",
        "        with open(f'{pretraining_dataset_path}/evidence/contexts_{chunk_id}.json', 'w') as f:\n",
        "            json.dump(ctx_output, f)\n",
        "\n",
        "    else:\n",
        "        evi_output = json.load(open(f'{pretraining_dataset_path}/evidence/evidence_pos_{chunk_id}.json'))"
      ],
      "metadata": {
        "id": "68mR5_dZvo3R",
        "outputId": "d09521c9-d45f-4198-e5cb-bf0d39470534",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}