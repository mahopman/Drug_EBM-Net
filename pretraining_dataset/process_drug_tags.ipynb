{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPh0IIOghrMebbC3CdsD45R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahopman/IEBM-Net/blob/main/pretraining_dataset/process_drug_tags.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = 'Qiao Jin'\n",
        "__editor__ = 'Mia Hopman'"
      ],
      "metadata": {
        "id": "AlNhWC_pzxmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_path = '/content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data'\n",
        "pretraining_dataset_path = f'{local_path}/pretraining_dataset'"
      ],
      "metadata": {
        "id": "gCfunpnbzyCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lw1pfg3-za4e"
      },
      "outputs": [],
      "source": [
        "def reversed(words, label):\n",
        "\n",
        "\tall_rev = (words + ['MASK'])[::-1]\n",
        "\n",
        "\tmask_idx = [idx for idx, word in enumerate(all_rev) if word == '[MASK]']\n",
        "\tmask_idx = [0] + mask_idx + [len(all_rev)]\n",
        "\n",
        "\tfor i, idx in enumerate(mask_idx[:-1]):\n",
        "\t\tall_rev[idx+1: mask_idx[i+1]] = all_rev[idx+1: mask_idx[i+1]][::-1]\n",
        "\n",
        "\tall_rev = all_rev[1:]\n",
        "\n",
        "\tif label in up2down:\n",
        "\t\trev_label = up2down[label]\n",
        "\telif label in down2up:\n",
        "\t\trev_label = down2up[label]\n",
        "\telse:\n",
        "\t\trev_label = label\n",
        "\n",
        "\treturn all_rev, rev_label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label(pos, indices, label2ctr):\n",
        "\tind_words = [pos[ind][0] for ind in indices]\n",
        "\n",
        "\tif len(ind_words) == 2:\n",
        "\t\tlabel = ind_words[0].lower()\n",
        "\t\tif label not in label2idx:\n",
        "\t\t\treturn False\n",
        "\t\telse:\n",
        "\t\t\tlabel2ctr[label] += 1\n",
        "\t\t\treturn label2idx[label]\n",
        "\telse:\n",
        "\t\tif ind_words[-1] == 'than':\n",
        "\t\t\treturn False\n",
        "\t\telse:\n",
        "\t\t\tlabel2ctr['nodiff'] += 1\n",
        "\t\t\treturn label2idx['nodiff']"
      ],
      "metadata": {
        "id": "rFr2j0b-zlOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# process labels\n",
        "ups = ['better', 'greater', 'higher', 'later', 'more', 'faster', 'older', 'longer', \\\n",
        "\t\t'larger', 'broader', 'wider', 'stronger', 'deeper', 'more', 'commoner', 'richer', \\\n",
        "\t\t'further', 'bigger']\n",
        "downs = ['worse', 'smaller', 'lower', 'earlier', 'less', 'slower', 'younger', 'shorter', \\\n",
        "\t\t'smaller', 'narrower', 'narrower', 'weaker', 'shallower', 'fewer', 'rarer', 'poorer', \\\n",
        "\t\t'closer', 'smaller']\n",
        "sims = ['nodiff', 'similar']\n",
        "\n",
        "label_set =  list(set(downs)) + list(set(sims)) + list(set(ups))\n",
        "\n",
        "label2idx = {label: idx for idx, label in enumerate(label_set)}\n",
        "label2idx['similarly'] = label2idx['similar']\n",
        "label2idx['similarity'] = label2idx['similar']\n",
        "label2idx['similarities'] = label2idx['similar']\n",
        "label2idx['farther'] = label2idx['further']\n",
        "\n",
        "label2ctr = {k: 0 for k in list(label2idx)}\n",
        "\n",
        "up2down = {label2idx[k]: label2idx[v] for k, v in zip(ups, downs)}\n",
        "down2up = {label2idx[k]: label2idx[v] for k, v in zip(downs, ups)}\n",
        "\n",
        "intervention2ids = json.load(open(f'{pretraining_dataset_path}/intervention2ids.json'))\n",
        "drug_pmids = intervention2ids['DRUG']\n",
        "\n",
        "# start\n",
        "output = []\n",
        "\n",
        "removed = set(['CD'])\n",
        "indicators = set(['significant', 'significantly', 'statistically', 'statistic', '%'])\n",
        "sims = set(['similar', 'similarly', 'similarity', 'similarities'])\n",
        "\n",
        "pmids = set()\n",
        "\n",
        "files = glob.glob(f'{pretraining_dataset_path}/evidence/evidence_pos_*.json')\n",
        "\n",
        "for file in files:\n",
        "    chunk_id = file.split('/')[-1].split('.')[0].split('_')[-1]\n",
        "    if not os.path.exists(file): continue\n",
        "    data = json.load(open(file))\n",
        "\n",
        "    for item in data:\n",
        "        pmid = item['pmid']\n",
        "        if pmid in drug_pmids:\n",
        "            pos = item['pos']\n",
        "            indices = item['indices']\n",
        "\n",
        "            label = get_label(pos, indices, label2ctr)\n",
        "\n",
        "            if not label: continue # lose about ~20%\n",
        "\n",
        "            par_stack = []\n",
        "            idx_stack = []\n",
        "            lefts = []\n",
        "            rights = []\n",
        "            for idx, info in enumerate(pos):\n",
        "                if info[0] in {'(', ')'}:\n",
        "                    if not par_stack:\n",
        "                        if info[0] == ')': continue\n",
        "                        par_stack.append(info[0])\n",
        "                        idx_stack.append(idx)\n",
        "                    else:\n",
        "                        if par_stack[-1] == info[0]:\n",
        "                            par_stack.append(info[0])\n",
        "                            idx_stack.append(idx)\n",
        "                        else:\n",
        "                            par_stack = par_stack[:-1]\n",
        "                            lefts.append(idx_stack[-1])\n",
        "                            rights.append(idx)\n",
        "                            idx_stack = idx_stack[:-1]\n",
        "\n",
        "            within_par = []\n",
        "            if lefts and rights:\n",
        "                for left, right in zip(lefts, rights):\n",
        "                    within_par += list(range(left, right+1))\n",
        "\n",
        "            # detect irrelavent subsentences\n",
        "            dot_indices = [idx for idx, info in enumerate(pos) if info[0] == ',']\n",
        "            outer_idx = []\n",
        "            if dot_indices:\n",
        "                left, right = min(item['indices']), max(item['indices'])\n",
        "                # item['indices'] save the important indices\n",
        "                dot_indices = [-1] + dot_indices + [len(pos)]\n",
        "                # print(left, right, dot_indices)\n",
        "                for i in range(len(dot_indices)-1):\n",
        "                    if dot_indices[i] <= left < dot_indices[i+1]:\n",
        "                        left_start = i\n",
        "                    if dot_indices[i] <= right < dot_indices[i+1]:\n",
        "                        right_start = i\n",
        "                left = dot_indices[left_start]\n",
        "                right = dot_indices[right_start+1]\n",
        "                for i in range(len(pos)):\n",
        "                    if i <= left or i >= right:\n",
        "                        outer_idx.append(i)\n",
        "\n",
        "\t\t    # detect irrelavent show that / suggest that\n",
        "\n",
        "            # RB before JJR in generally bad\n",
        "            include_idx = []\n",
        "            that_judged = False # only judge once\n",
        "            for idx, i in enumerate(pos):\n",
        "                if idx in outer_idx:\n",
        "                    #print(i, '----------OUT')\n",
        "                    pass\n",
        "                elif idx+1 < len(pos) and (pos[idx+1][1] == 'JJR' or pos[idx+1][1] == 'RBR') and \\\n",
        "                    ((i[1] == 'RB' and i[0].lower() != 'not') \\\n",
        "                    or i[0].lower() == 'times'):\n",
        "                    #print(i, '----------FRONT_RB')\n",
        "                    pass\n",
        "                elif idx in item['indices']:\n",
        "                    #print(i, '----------DETECTED')\n",
        "                    pass\n",
        "                elif i[1] in removed:\n",
        "                    #print(i, '----------TOREMOVE')\n",
        "                    pass\n",
        "                elif i[0].lower() in indicators:\n",
        "                    #print(i, '----------INDICATOR')\n",
        "                    pass\n",
        "                elif idx in within_par:\n",
        "                    #print(i, '----------INPAR')\n",
        "                    pass\n",
        "                elif not that_judged and i[0].lower() == 'that':\n",
        "                    if idx < min(item['indices']):\n",
        "                        #print(i, '----------THAT')\n",
        "                        that_judged = True\n",
        "                        include_idx = []\n",
        "                else:\n",
        "                    #print(i)\n",
        "                    include_idx.append(idx)\n",
        "\n",
        "            final_evidence = []\n",
        "            for idx, i in enumerate(pos):\n",
        "\n",
        "                if idx in include_idx:\n",
        "                    final_evidence.append(i[0])\n",
        "                else:\n",
        "                    if  final_evidence and final_evidence[-1] != '[MASK]':\n",
        "                        final_evidence.append('[MASK]')\n",
        "\n",
        "            if not final_evidence: continue\n",
        "\n",
        "            if final_evidence[-1] in ['.', '[MASK]']:\n",
        "                final_evidence = final_evidence[:-1]\n",
        "\n",
        "            # Make every word after [MASK] upper cased\n",
        "            for idx, word in enumerate(final_evidence):\n",
        "                if idx == 0 and word != '[MASK]':\n",
        "                    final_evidence[idx] = word[0].upper() + word[1:]\n",
        "                elif word == '[MASK]' and idx + 1 < len(final_evidence) and final_evidence[idx+1]:\n",
        "                    final_evidence[idx+1] = final_evidence[idx+1][0].upper() + final_evidence[idx+1][1:]\n",
        "\n",
        "            rev_evidence, rev_label = reversed(final_evidence, label)\n",
        "\n",
        "            output.append({'pmid': pmid,\n",
        "                    'pico': ' '.join(final_evidence), 'label': label,\n",
        "                    'rev_pico': ' '.join(rev_evidence), 'rev_label': rev_label})\n",
        "\n",
        "            pmids.add(pmid)\n",
        "\n",
        "\tprint('Processed chunk #%d. Got %d insts' % (chunk_id, len(output)))\n",
        "\n",
        "with open(f'{pretraining_dataset_path}/evidence.json', 'w') as f:\n",
        "\tjson.dump(output, f, indent=4)\n",
        "\n",
        "with open(f'{pretraining_dataset_path}/evidence_pmids.json', 'w') as f:\n",
        "\tjson.dump(list(pmids), f, indent=4)"
      ],
      "metadata": {
        "id": "rF1pg59mznyo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}