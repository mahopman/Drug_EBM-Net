{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "https://github.com/mahopman/IEBM-Net/blob/main/run_drug_ebmnet.ipynb",
      "authorship_tag": "ABX9TyN+MiUxnttdTNtdNmVyEt9Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahopman/IEBM-Net/blob/main/run_drug_ebmnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0.\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ],
      "metadata": {
        "id": "TRGDrFHjRPaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = 'Qiao Jin'\n",
        "__editor__ = 'Mia Hopman'"
      ],
      "metadata": {
        "id": "OV-YG4YERJqZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_path = '/content/drive/MyDrive/MS_DataScience/DS595/IEBM-Net_Data'\n",
        "pretraining_dataset_path = f'{local_path}/pretraining_dataset'\n",
        "evidence_integration_path = f'{local_path}/evidence_integration'"
      ],
      "metadata": {
        "id": "iOUUm1rFR0RJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Yyz64SxaQtSc"
      },
      "outputs": [],
      "source": [
        "import random as rd\n",
        "import torch\n",
        "\n",
        "def set_seed(args):\n",
        "\trd.seed(args.seed)\n",
        "\tnp.random.seed(args.seed)\n",
        "\ttorch.manual_seed(args.seed)\n",
        "\tif args.n_gpu > 0:\n",
        "\t\ttorch.cuda.manual_seed_all(args.seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_list(tensor):\n",
        "\treturn tensor.detach().cpu().tolist()"
      ],
      "metadata": {
        "id": "YOuW8JnqRVRU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "from transformers import AdamW, get_cosine_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "def train(args, train_picos, train_ctxs, model, tokenizer):\n",
        "\t\"\"\" Train the model \"\"\"\n",
        "\t#tb_writer = SummaryWriter()\n",
        "\n",
        "\targs.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "\ttrain_sampler = RandomSampler(train_picos)\n",
        "\ttrain_dataloader = DataLoader(train_picos, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "\tif args.max_steps > 0:\n",
        "\t\tt_total = args.max_steps\n",
        "\t\targs.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "\telse:\n",
        "\t\tt_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "\t# Prepare optimizer and schedule (linear warmup and decay)\n",
        "\tno_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "\toptimizer_grouped_parameters = [\n",
        "\t\t{\n",
        "\t\t\t\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "\t\t\t\"weight_decay\": args.weight_decay,\n",
        "\t\t},\n",
        "\t\t{\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "\t]\n",
        "\toptimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "\tscheduler = get_cosine_schedule_with_warmup(\n",
        "\t\toptimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        "\t)\n",
        "\n",
        "\t# multi-gpu training\n",
        "\tif args.n_gpu > 1:\n",
        "\t\tmodel = torch.nn.DataParallel(model)\n",
        "\n",
        "\t# Train!\n",
        "\tlogger.info(\"***** Running training *****\")\n",
        "\tlogger.info(\"  Num examples = %d\", len(train_picos))\n",
        "\tlogger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "\tlogger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "\tlogger.info(\n",
        "\t\t\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "\t\targs.train_batch_size\n",
        "\t\t* args.gradient_accumulation_steps)\n",
        "\tlogger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "\tlogger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "\tglobal_step = 0\n",
        "\ttr_loss, logging_loss = 0.0, 0.0\n",
        "\tmodel.zero_grad()\n",
        "\ttrain_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=False)\n",
        "\tset_seed(args)\t# Added here for reproductibility\n",
        "\tfor _ in train_iterator:\n",
        "\t\tepoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=False)\n",
        "\t\tfor step, batch in enumerate(epoch_iterator):\n",
        "\t\t\tmodel.train()\n",
        "\n",
        "\t\t\tbatch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "\t\t\tctx_ids = to_list(batch[0])\n",
        "\t\t\tpico_token_ids = batch[1] # B x max_pico_length\n",
        "\t\t\tpico_token_mask = batch[2] # B x max_pico_length\n",
        "\t\t\tpico_segment_ids = batch[3] # B x max_pico_length\n",
        "\t\t\tlabels = batch[4]\n",
        "\n",
        "\t\t\tctx_batch = [train_ctxs[ctx_id] for ctx_id in ctx_ids] # B x list of ctx dataset\n",
        "\t\t\tctx_batch = list(map(list, zip(*ctx_batch)))\n",
        "\n",
        "\t\t\tctx_token_ids = torch.stack(ctx_batch[1]).to(args.device) # B x max_ctx_length\n",
        "\t\t\tctx_token_mask = torch.stack(ctx_batch[2]).to(args.device) # B x max_ctx_length\n",
        "\t\t\tctx_segment_ids = torch.stack(ctx_batch[3]).to(args.device) # B x max_ctx_length\n",
        "\n",
        "\t\t\tinputs = {\n",
        "\t\t\t\t\"passage_ids\": torch.cat([ctx_token_ids, pico_token_ids], dim=1),\n",
        "\t\t\t\t\"passage_mask\": torch.cat([ctx_token_mask, pico_token_mask], dim=1),\n",
        "\t\t\t\t\"passage_segment_ids\": torch.cat([ctx_segment_ids, pico_segment_ids], dim=1),\n",
        "\t\t\t\t\"result_labels\": labels\n",
        "\t\t\t}\n",
        "\n",
        "\t\t\toutputs = model(inputs)\n",
        "\n",
        "\t\t\tloss = outputs  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "\t\t\tif args.n_gpu > 1:\n",
        "\t\t\t\tloss = loss.mean()\t# mean() to average on multi-gpu parallel (not distributed) training\n",
        "\t\t\tif args.gradient_accumulation_steps > 1:\n",
        "\t\t\t\tloss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "\t\t\tloss.backward()\n",
        "\n",
        "\t\t\ttr_loss += loss.item()\n",
        "\n",
        "\t\t\tif (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "\t\t\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "\t\t\t\toptimizer.step()\n",
        "\t\t\t\tscheduler.step()  # Update learning rate schedule\n",
        "\t\t\t\tmodel.zero_grad()\n",
        "\t\t\t\tglobal_step += 1\n",
        "\n",
        "\t\t\t\tif args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "\t\t\t\t\t# Log metrics\n",
        "\t\t\t\t\t#tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
        "\t\t\t\t\t#tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
        "\t\t\t\t\t#print((tr_loss - logging_loss) / args.logging_steps)\n",
        "\t\t\t\t\tlogging_loss = tr_loss\n",
        "\n",
        "\t\t\t\tif args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "\t\t\t\t\t# Save model checkpoint\n",
        "\t\t\t\t\toutput_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
        "\t\t\t\t\tif not os.path.exists(output_dir):\n",
        "\t\t\t\t\t\tos.makedirs(output_dir)\n",
        "\t\t\t\t\tmodel_to_save = (\n",
        "\t\t\t\t\t\tmodel.module if hasattr(model, \"module\") else model\n",
        "\t\t\t\t\t)  # Take care of distributed/parallel training\n",
        "\t\t\t\t\tmodel_to_save.save_pretrained(output_dir)\n",
        "\t\t\t\t\ttokenizer.save_pretrained(output_dir)\n",
        "\t\t\t\t\ttorch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "\t\t\t\t\tlogger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "\t\t\tif args.max_steps > 0 and global_step > args.max_steps:\n",
        "\t\t\t\tepoch_iterator.close()\n",
        "\t\t\t\tbreak\n",
        "\t\tif args.max_steps > 0 and global_step > args.max_steps:\n",
        "\t\t\ttrain_iterator.close()\n",
        "\t\t\tbreak\n",
        "\n",
        "\t#tb_writer.close()\n",
        "\n",
        "\treturn global_step, tr_loss / global_step"
      ],
      "metadata": {
        "id": "IjTBKITMRV6v"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from torch.utils.data import SequentialSampler\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def evaluate(args, eval_picos, eval_ctxs, model, tokenizer, prefix=\"\"):\n",
        "\n",
        "\tif not os.path.exists(args.output_dir):\n",
        "\t\tos.makedirs(args.output_dir)\n",
        "\n",
        "\targs.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "\t# Note that DistributedSampler samples randomly\n",
        "\teval_sampler = SequentialSampler(eval_picos)\n",
        "\teval_dataloader = DataLoader(eval_picos, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "\t# Eval!\n",
        "\tlogger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "\tlogger.info(\"  Num examples = %d\", len(eval_picos))\n",
        "\tlogger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "\n",
        "\texample_ids = []\n",
        "\tall_labels = []\n",
        "\tall_preds = []\n",
        "\tall_logits = np.zeros((0, 3))\n",
        "\n",
        "\tfor batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "\t\tmodel.eval()\n",
        "\t\tbatch = tuple(t.to(args.device) for t in batch)\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tctx_ids = to_list(batch[0])\n",
        "\t\t\tpico_token_ids = batch[1] # B x max_pico_length\n",
        "\t\t\tpico_token_mask = batch[2] # B x max_pico_length\n",
        "\t\t\tpico_segment_ids = batch[3] # B x max_pico_length\n",
        "\t\t\tlabels = batch[4]\n",
        "\n",
        "\t\t\tctx_batch = [eval_ctxs[ctx_id] for ctx_id in ctx_ids] # B x list of ctx dataset\n",
        "\t\t\tctx_batch = list(map(list, zip(*ctx_batch)))\n",
        "\n",
        "\t\t\tctx_token_ids = torch.stack(ctx_batch[1]).to(args.device) # B x max_ctx_length\n",
        "\t\t\tctx_token_mask = torch.stack(ctx_batch[2]).to(args.device) # B x max_ctx_length\n",
        "\t\t\tctx_segment_ids = torch.stack(ctx_batch[3]).to(args.device) # B x max_ctx_length\n",
        "\n",
        "\t\t\tinputs = {\n",
        "\t\t\t\t\"passage_ids\": torch.cat([ctx_token_ids, pico_token_ids], dim=1),\n",
        "\t\t\t\t\"passage_mask\": torch.cat([ctx_token_mask, pico_token_mask], dim=1),\n",
        "\t\t\t\t\"passage_segment_ids\": torch.cat([ctx_segment_ids, pico_segment_ids], dim=1)\n",
        "\t\t\t}\n",
        "\n",
        "\t\t\tlogits = model(inputs) # N x 3\n",
        "\t\t\tpreds = torch.argmax(logits, dim=1) # N\n",
        "\n",
        "\t\t\texample_ids += list(batch[4].detach().cpu().numpy())\n",
        "\t\t\tall_labels += list(labels.detach().cpu().numpy())\n",
        "\t\t\tall_preds += list(preds.detach().cpu().numpy())\n",
        "\t\t\tall_logits = np.concatenate([all_logits, logits.detach().cpu().numpy()], axis=0)\n",
        "\n",
        "\tif not prefix:\n",
        "\t\tprefix = 'final'\n",
        "\n",
        "\twith open(os.path.join(args.output_dir, '%s_all_example_idx.json' % prefix), 'w') as f:\n",
        "\t\tjson.dump([int(label) for label in example_ids], f)\n",
        "\twith open(os.path.join(args.output_dir, '%s_all_labels.json' % prefix), 'w') as f:\n",
        "\t\tjson.dump([int(label) for label in all_labels], f)\n",
        "\twith open(os.path.join(args.output_dir, '%s_all_preds.json' % prefix), 'w') as f:\n",
        "\t\tjson.dump([int(pred) for pred in all_preds], f)\n",
        "\tnp.save(os.path.join(args.output_dir, '%s_all_logits.npy' % prefix), np.array(all_logits))\n",
        "\n",
        "\tresults = {}\n",
        "\tresults['f1'] = f1_score(all_labels, all_preds, average='macro')\n",
        "\tresults['acc'] = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "\treturn results"
      ],
      "metadata": {
        "id": "QH1bBky9RgN4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def represent(args, model, tokenizer):\n",
        "\tdataset = load_and_cache_examples(args, tokenizer, evaluate=True, do_repr=True)\n",
        "\n",
        "\tif not os.path.exists(args.output_dir):\n",
        "\t\tos.makedirs(args.output_dir)\n",
        "\n",
        "\targs.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "\t# Note that DistributedSampler samples randomly\n",
        "\teval_sampler = SequentialSampler(dataset)\n",
        "\teval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "\t# Eval!\n",
        "\tlogger.info(\"***** Running Representations *****\")\n",
        "\tlogger.info(\"  Num examples = %d\", len(dataset))\n",
        "\tlogger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "\n",
        "\texample_ids = []\n",
        "\tall_reprs = np.zeros((0, model.bert.config.hidden_size))\n",
        "\n",
        "\tfor batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "\t\tmodel.eval()\n",
        "\t\tbatch = tuple(t.to(args.device) for t in batch)\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tinputs = {\n",
        "\t\t\t\t\"passage_ids\": batch[0],\n",
        "\t\t\t\t\"passage_mask\": batch[1],\n",
        "\t\t\t\t\"passage_segment_ids\": batch[2],\n",
        "\t\t\t}\n",
        "\n",
        "\t\t\treprs = model(inputs, get_reprs=True) # N x D\n",
        "\n",
        "\t\t\texample_ids += list(batch[4].detach().cpu().numpy())\n",
        "\t\t\tall_reprs = np.concatenate([all_reprs, reprs.detach().cpu().numpy()], axis=0)\n",
        "\n",
        "\twith open(os.path.join(args.output_dir, 'all_example_idx.json'), 'w') as f:\n",
        "\t\tjson.dump([int(_id) for _id in example_ids], f)\n",
        "\tnp.save(os.path.join(args.output_dir, 'all_reprs.npy'), np.array(all_reprs))"
      ],
      "metadata": {
        "id": "JfhyGQL9TaNr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CtxFeatures(object): # same with pre-training utils\n",
        "\t\"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "\tdef __init__(\n",
        "\t\tself,\n",
        "\t\tctx_id,\n",
        "\t\ttokens,\n",
        "\t\tinput_ids,\n",
        "\t\tinput_mask,\n",
        "\t\tsegment_ids\n",
        "\t):\n",
        "\t\tself.ctx_id = ctx_id\n",
        "\t\tself.tokens = tokens\n",
        "\t\tself.input_ids = input_ids\n",
        "\t\tself.input_mask = input_mask\n",
        "\t\tself.segment_ids = segment_ids"
      ],
      "metadata": {
        "id": "ArjPCRhNTctF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_ctxs_to_features(\n",
        "\texamples,\n",
        "\ttokenizer,\n",
        "\tmax_passage_length,\n",
        "\tpermutation=None,\n",
        "\tcls_token=\"[CLS]\",\n",
        "\tsep_token=\"[SEP]\",\n",
        "\tpad_token=0,\n",
        "\tsequence_a_segment_id=0,\n",
        "\tsequence_b_segment_id=1,\n",
        "\tcls_token_segment_id=0,\n",
        "\tpad_token_segment_id=0\n",
        "):\n",
        "\t\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "\tfeatures = []\n",
        "\tfor example in tqdm(examples):\n",
        "\t\tctx_id = example.ctx_id\n",
        "\t\tpsg_tokens = tokenizer.tokenize(example.passage_text)\n",
        "\n",
        "\t\ttokens = []\n",
        "\t\tsegment_ids = []\n",
        "\t\tinput_mask = []\n",
        "\n",
        "\t\ttokens += [cls_token]\n",
        "\t\ttokens += psg_tokens[:max_passage_length - 2]\n",
        "\t\ttokens += [sep_token]\n",
        "\n",
        "\t\tsegment_ids = [sequence_a_segment_id] * len(tokens)\n",
        "\n",
        "\t\tinput_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\t\tinput_mask = [1] * len(input_ids)\n",
        "\n",
        "\t\t# Zero-pad up to the sequence length.\n",
        "\t\twhile len(input_ids) < max_passage_length:\n",
        "\t\t\tinput_ids.append(pad_token)\n",
        "\t\t\tinput_mask.append(0)\n",
        "\t\t\tsegment_ids.append(pad_token_segment_id)\n",
        "\n",
        "\t\tassert len(input_ids) == max_passage_length\n",
        "\t\tassert len(input_mask) == max_passage_length\n",
        "\t\tassert len(segment_ids) == max_passage_length\n",
        "\n",
        "\t\tif ctx_id < 20:\n",
        "\t\t\tlogger.info(\"*** Example ***\")\n",
        "\t\t\tlogger.info(\"ctx_id: %s\" % (ctx_id))\n",
        "\t\t\tlogger.info(\"tokens: %s\" % \" \".join(tokens))\n",
        "\t\t\tlogger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "\t\t\tlogger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "\t\t\tlogger.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "\n",
        "\t\tfeatures.append(\n",
        "\t\t\tCtxFeatures(\n",
        "\t\t\t\tctx_id=ctx_id,\n",
        "\t\t\t\ttokens=tokens,\n",
        "\t\t\t\tinput_ids=input_ids,\n",
        "\t\t\t\tinput_mask=input_mask,\n",
        "\t\t\t\tsegment_ids=segment_ids\n",
        "\t\t\t)\n",
        "\t\t)\n",
        "\n",
        "\treturn features"
      ],
      "metadata": {
        "id": "5yQwA4Y1Tg5z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CtxExample(object): # same with pre-training utils\n",
        "\t\"\"\"\n",
        "\ta single training/test example for the EBM-Net dataset.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(\n",
        "\t\tself,\n",
        "\t\tctx_id,\n",
        "\t\tpassage_text\n",
        "\t):\n",
        "\t\tself.ctx_id = ctx_id\n",
        "\t\tself.passage_text = passage_text\n",
        "\n",
        "\tdef __str__(self):\n",
        "\t\treturn self.__repr__()\n",
        "\n",
        "\tdef __repr__(self):\n",
        "\t\ts = \"\"\n",
        "\t\ts += 'ctx_id: %s\\n' % self.ctx_id\n",
        "\t\ts += \"passage: %s\\n\" % self.passage_text\n",
        "\n",
        "\t\treturn s"
      ],
      "metadata": {
        "id": "DZrLhSdfThcu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_ctx_examples(input_file, adversarial=False): # same with pre-training utils\n",
        "\t\"\"\"Read a EBM-Net json file into a list of EbmExample.\"\"\"\n",
        "\twith open(input_file, \"r\", encoding=\"utf-8\") as reader:\n",
        "\t\tinput_data = json.load(reader)\n",
        "\n",
        "\texamples = []\n",
        "\n",
        "\tfor entry in input_data:\n",
        "\t\texample = CtxExample(\n",
        "\t\t\tctx_id=entry['ctx_id'],\n",
        "\t\t\tpassage_text=entry['passage']\n",
        "\t\t)\n",
        "\t\texamples.append(example)\n",
        "\n",
        "\treturn examples"
      ],
      "metadata": {
        "id": "2QlY6HQPTjm9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "def load_and_cache_ctxs(args, tokenizer, evaluate=False, do_repr=False, pretraining=False):\n",
        "\t# We need to index it\n",
        "\n",
        "\t# Load data features from cache or dataset file\n",
        "\tif do_repr:\n",
        "\t\tinput_file = args.repr_ctx\n",
        "\telse:\n",
        "\t\tinput_file = args.predict_ctx if evaluate else args.train_ctx\n",
        "\n",
        "\tcached_features_file = os.path.join(\n",
        "\t\tos.path.dirname(input_file),\n",
        "\t\t\"cached_ctxs_adv{}_{}_{}\".format(\n",
        "\t\t\targs.adversarial,\n",
        "\t\t\t\"dev\" if evaluate else \"train\",\n",
        "\t\t\tstr(args.max_passage_length)\n",
        "\t\t),\n",
        "\t)\n",
        "\n",
        "\tif os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
        "\t\tlogger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "\t\tfeatures = torch.load(cached_features_file)\n",
        "\telse:\n",
        "\t\tlogger.info(\"Creating features from dataset file at %s\", input_file)\n",
        "\n",
        "\t\texamples = read_ctx_examples(input_file=input_file, adversarial=args.adversarial)\n",
        "\n",
        "\t\tfeatures = convert_ctxs_to_features(\n",
        "\t\t\texamples=examples,\n",
        "\t\t\ttokenizer=tokenizer,\n",
        "\t\t\tmax_passage_length=args.max_passage_length\n",
        "\t\t)\n",
        "\n",
        "\t\tlogger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "\t\ttorch.save(features, cached_features_file)\n",
        "\n",
        "\t# Convert to Tensors and build dataset\n",
        "\tall_ctx_ids = torch.tensor([f.ctx_id for f in features], dtype=torch.long)\n",
        "\tall_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "\tall_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "\tall_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "\n",
        "\tdataset = TensorDataset(\n",
        "\t\tall_ctx_ids,\n",
        "\t\tall_input_ids,\n",
        "\t\tall_input_mask,\n",
        "\t\tall_segment_ids\n",
        "\t)\n",
        "\n",
        "\treturn dataset"
      ],
      "metadata": {
        "id": "kF8F5zxTTlrB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PicoFeatures(object): # unchanged from pre-training utils\n",
        "\t\"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "\tdef __init__(\n",
        "\t\tself,\n",
        "\t\texample_index,\n",
        "\t\tctx_id,\n",
        "\t\ttokens,\n",
        "\t\tinput_ids,\n",
        "\t\tinput_mask,\n",
        "\t\tsegment_ids,\n",
        "\t\tlabel\n",
        "\t):\n",
        "\t\tself.example_index = example_index\n",
        "\t\tself.ctx_id = ctx_id\n",
        "\t\tself.tokens = tokens\n",
        "\t\tself.input_ids = input_ids\n",
        "\t\tself.input_mask = input_mask\n",
        "\t\tself.segment_ids = segment_ids\n",
        "\t\tself.label = label"
      ],
      "metadata": {
        "id": "hKvx6a3YTpoa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_picos_to_features_pretraining(\n",
        "\texamples,\n",
        "\ttokenizer,\n",
        "\tmax_pico_length,\n",
        "\tpermutation=None,\n",
        "\tcls_token=\"[CLS]\",\n",
        "\tsep_token=\"[SEP]\",\n",
        "\tpad_token=0,\n",
        "\tsequence_a_segment_id=0,\n",
        "\tsequence_b_segment_id=1,\n",
        "\tcls_token_segment_id=0,\n",
        "\tpad_token_segment_id=0\n",
        "):\n",
        "\t\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "\tfeatures = []\n",
        "\texample_index = 0\n",
        "\n",
        "\tfor example in examples:\n",
        "\t\tctx_id = example.ctx_id\n",
        "\n",
        "\t\tpico_tokens = tokenizer.tokenize(example.pico_text)\n",
        "\n",
        "\t\ttokens = []\n",
        "\t\tsegment_ids = []\n",
        "\t\tinput_mask = []\n",
        "\t\tlabel = example.label\n",
        "\n",
        "\t\ttokens += pico_tokens[:max_pico_length-1] + [sep_token]\n",
        "\t\tsegment_ids = [sequence_b_segment_id] * len(tokens)\n",
        "\n",
        "\t\tinput_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\t\tinput_mask = [1] * len(input_ids)\n",
        "\n",
        "\t\t# Zero-pad up to the sequence length.\n",
        "\t\twhile len(input_ids) < max_pico_length:\n",
        "\t\t\tinput_ids.append(pad_token)\n",
        "\t\t\tinput_mask.append(0)\n",
        "\t\t\tsegment_ids.append(pad_token_segment_id)\n",
        "\n",
        "\t\tassert len(input_ids) == max_pico_length\n",
        "\t\tassert len(input_mask) == max_pico_length\n",
        "\t\tassert len(segment_ids) == max_pico_length\n",
        "\n",
        "\t\tif example_index < 20:\n",
        "\t\t\tlogger.info(\"*** Example ***\")\n",
        "\t\t\tlogger.info(\"ctx_id: %s\" % (ctx_id))\n",
        "\t\t\tlogger.info(\"example_index: %s\" % (example_index))\n",
        "\t\t\tlogger.info(\"tokens: %s\" % \" \".join(tokens))\n",
        "\t\t\tlogger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "\t\t\tlogger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "\t\t\tlogger.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "\t\t\tlogger.info(\"label: %s\" % label)\n",
        "\n",
        "\t\tfeatures.append(\n",
        "\t\t\tPicoFeatures(\n",
        "\t\t\t\tctx_id=ctx_id,\n",
        "\t\t\t\texample_index=example_index,\n",
        "\t\t\t\ttokens=tokens,\n",
        "\t\t\t\tinput_ids=input_ids,\n",
        "\t\t\t\tinput_mask=input_mask,\n",
        "\t\t\t\tsegment_ids=segment_ids,\n",
        "\t\t\t\tlabel=label\n",
        "\t\t\t)\n",
        "\t\t)\n",
        "\n",
        "\t\texample_index += 1\n",
        "\n",
        "\treturn features"
      ],
      "metadata": {
        "id": "W-1hCKtETsRJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_picos_to_features_ebmnet(\n",
        "\texamples,\n",
        "\ttokenizer,\n",
        "\tmax_pico_length,\n",
        "\tpermutation=None,\n",
        "\tcls_token=\"[CLS]\",\n",
        "\tsep_token=\"[SEP]\",\n",
        "\tpad_token=0,\n",
        "\tsequence_a_segment_id=0,\n",
        "\tsequence_b_segment_id=1,\n",
        "\tcls_token_segment_id=0,\n",
        "\tpad_token_segment_id=0\n",
        "):\n",
        "\t\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "\tfeatures = []\n",
        "\texample_index = 0\n",
        "\n",
        "\tif '-' in permutation: # shifting\n",
        "\t\tperm_list = permutation.split('-')\n",
        "\telse:\n",
        "\t\tperm_list = [permutation]\n",
        "\n",
        "\tfor perm in perm_list:\n",
        "\t\tfor (example_index, example) in enumerate(examples):\n",
        "\t\t\tctx_id = example.ctx_id\n",
        "\n",
        "\t\t\ti_tokens = tokenizer.tokenize(example.i_text)\n",
        "\t\t\tc_tokens = tokenizer.tokenize(example.c_text)\n",
        "\t\t\to_tokens = tokenizer.tokenize(example.o_text)\n",
        "\t\t\tico_tokens = {'i': i_tokens,\n",
        "\t\t\t\t\t\t  'c': c_tokens,\n",
        "\t\t\t\t\t\t  'o': o_tokens}\n",
        "\n",
        "\t\t\ttokens = []\n",
        "\t\t\tsegment_ids = []\n",
        "\t\t\tinput_mask = []\n",
        "\t\t\tlabel = example.label\n",
        "\n",
        "\t\t\tassert set(perm).issubset({'i', 'o', 'c'})\n",
        "\t\t\tfor element in perm:\n",
        "\t\t\t\ttokens += ico_tokens[element] + ['[MASK]']\n",
        "\t\t\ttokens[-1] = sep_token\n",
        "\t\t\tsegment_ids = [sequence_b_segment_id] * len(tokens)\n",
        "\n",
        "\t\t\tif len(tokens) > max_pico_length:\n",
        "\t\t\t\ttokens = tokens[:max_pico_length-1] + [sep_token]\n",
        "\t\t\t\tsegment_ids = segment_ids[:max_pico_length-1] + [sequence_b_segment_id]\n",
        "\n",
        "\t\t\tinput_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\t\t\tinput_mask = [1] * len(input_ids)\n",
        "\n",
        "\t\t\t# Zero-pad up to the sequence length.\n",
        "\t\t\twhile len(input_ids) < max_pico_length:\n",
        "\t\t\t\tinput_ids.append(pad_token)\n",
        "\t\t\t\tinput_mask.append(0)\n",
        "\t\t\t\tsegment_ids.append(pad_token_segment_id)\n",
        "\n",
        "\t\t\tassert len(input_ids) == max_pico_length\n",
        "\t\t\tassert len(input_mask) == max_pico_length\n",
        "\t\t\tassert len(segment_ids) == max_pico_length\n",
        "\n",
        "\t\t\tif example_index < 20:\n",
        "\t\t\t\tlogger.info(\"*** Example ***\")\n",
        "\t\t\t\tlogger.info(\"example_index: %s\" % (example_index))\n",
        "\t\t\t\tlogger.info(\"tokens: %s\" % \" \".join(tokens))\n",
        "\t\t\t\tlogger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "\t\t\t\tlogger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "\t\t\t\tlogger.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "\n",
        "\t\t\tfeatures.append(\n",
        "\t\t\t\tPicoFeatures(\n",
        "\t\t\t\t\tctx_id=ctx_id,\n",
        "\t\t\t\t\texample_index=example_index,\n",
        "\t\t\t\t\ttokens=tokens,\n",
        "\t\t\t\t\tinput_ids=input_ids,\n",
        "\t\t\t\t\tinput_mask=input_mask,\n",
        "\t\t\t\t\tsegment_ids=segment_ids,\n",
        "\t\t\t\t\tlabel=label\n",
        "\t\t\t\t)\n",
        "\t\t\t)\n",
        "\n",
        "\treturn features"
      ],
      "metadata": {
        "id": "4WZSzFexT4F7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PicoExamplePretraining(object):\n",
        "\t\"\"\"\n",
        "\ta single training/test example for the EBM-Net dataset.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(\n",
        "\t\tself,\n",
        "\t\tctx_id,\n",
        "\t\tpico_text,\n",
        "\t\tlabel\n",
        "\t):\n",
        "\t\tself.ctx_id = ctx_id\n",
        "\t\tself.pico_text = pico_text\n",
        "\t\tself.label = label\n",
        "\n",
        "\tdef __str__(self):\n",
        "\t\treturn self.__repr__()\n",
        "\n",
        "\tdef __repr__(self):\n",
        "\t\ts = \"\"\n",
        "\t\ts += \"ctx_id: %s\\n\" % self.ctx_id\n",
        "\t\ts += \"pico_text: %s\\n\" % self.pico_text\n",
        "\t\ts += \"label: %s\\n\" % self.label\n",
        "\n",
        "\t\treturn s"
      ],
      "metadata": {
        "id": "8iX8PM8hT4rb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pico_examples_pretraining(input_file, adversarial=False):\n",
        "\t\"\"\"Read a EBM-Net json file into a list of EbmExample.\"\"\"\n",
        "\twith open(input_file, \"r\", encoding=\"utf-8\") as reader:\n",
        "\t\tinput_data = json.load(reader)\n",
        "\n",
        "\texamples = []\n",
        "\n",
        "\tfor entry in input_data:\n",
        "\t\texample = PicoExamplePretraining(\n",
        "\t\t\tctx_id=entry['ctx_id'],\n",
        "\t\t\tpico_text=entry['pico'],\n",
        "\t\t\tlabel=entry['label']\n",
        "\t\t)\n",
        "\t\texamples.append(example)\n",
        "\n",
        "\t\tif adversarial:\n",
        "\t\t\texample = PicoExamplePretraining(\n",
        "\t\t\t\tctx_id=entry['ctx_id'],\n",
        "\t\t\t\tpico_text=entry['rev_pico'],\n",
        "\t\t\t\tlabel=entry['rev_label']\n",
        "\t\t\t)\n",
        "\t\t\texamples.append(example)\n",
        "\n",
        "\treturn examples"
      ],
      "metadata": {
        "id": "3Dx02TUiVa3z"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PicoExampleEBMNet(object):\n",
        "\t\"\"\"\n",
        "\ta single training/test example for the EBM-Net dataset.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(\n",
        "\t\tself,\n",
        "\t\tctx_id,\n",
        "\t\ti_text,\n",
        "\t\tc_text,\n",
        "\t\to_text,\n",
        "\t\tlabel\n",
        "\t):\n",
        "\t\tself.ctx_id = ctx_id\n",
        "\t\tself.i_text = i_text\n",
        "\t\tself.c_text = c_text\n",
        "\t\tself.o_text = o_text\n",
        "\t\tself.label = label\n",
        "\n",
        "\tdef __str__(self):\n",
        "\t\treturn self.__repr__()\n",
        "\n",
        "\tdef __repr__(self):\n",
        "\t\ts = \"\"\n",
        "\t\ts += \"ctx_id: %s\\n\" % self.ctx_id\n",
        "\t\ts += \"i_text: %s\\n\" % self.i_text\n",
        "\t\ts += \"c_text: %s\\n\" % self.c_text\n",
        "\t\ts += \"o_text: %s\\n\" % self.o_text\n",
        "\t\ts += \"label: %s\\n\" % self.label\n",
        "\n",
        "\t\treturn s"
      ],
      "metadata": {
        "id": "rn1S1zmVVbXU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pico_examples_ebmnet(input_file, adversarial=False):\n",
        "\t\"\"\"Read a EBM-Net json file into a list of EbmExample.\"\"\"\n",
        "\twith open(input_file, \"r\", encoding=\"utf-8\") as reader:\n",
        "\t\tinput_data = json.load(reader)\n",
        "\n",
        "\texamples = []\n",
        "\n",
        "\tfor entry in input_data:\n",
        "\t\texample = PicoExampleEBMNet(\n",
        "\t\t\tctx_id=entry['ctx_id'],\n",
        "\t\t\ti_text=entry['i_text'],\n",
        "\t\t\tc_text=entry['c_text'],\n",
        "\t\t\to_text=entry['o_text'],\n",
        "\t\t\tlabel=entry['label']\n",
        "\t\t)\n",
        "\t\texamples.append(example)\n",
        "\n",
        "\t\tif adversarial:\n",
        "\t\t\texample = PicoExampleEBMNet(\n",
        "\t\t\t\tctx_id=entry['ctx_id'],\n",
        "\t\t\t\ti_text=entry['c_text'],\n",
        "\t\t\t\tc_text=entry['i_text'],\n",
        "\t\t\t\to_text=entry['o_text'],\n",
        "\t\t\t\tlabel=2-entry['label']\n",
        "\t\t\t)\n",
        "\t\t\texamples.append(example)\n",
        "\n",
        "\treturn examples"
      ],
      "metadata": {
        "id": "jR2eaA3UVeJs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_cache_picos(args, tokenizer, evaluate=False, do_repr=False, pretraining=False):\n",
        "\t# Dataset that we are going to use\n",
        "\n",
        "\t# Load data features from cache or dataset file\n",
        "    if do_repr:\n",
        "        input_file = args.repr_pico\n",
        "    else:\n",
        "        input_file = args.predict_pico if evaluate else args.train_pico\n",
        "\n",
        "    cached_features_file = os.path.join(\n",
        "        os.path.dirname(input_file),\n",
        "        \"cached_picos_adv{}_{}_{}_{}\".format(\n",
        "            args.adversarial,\n",
        "            args.permutation,\n",
        "            \"dev\" if evaluate else \"train\",\n",
        "            str(args.max_pico_length)\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", input_file)\n",
        "\n",
        "        if args.pretraining:\n",
        "            examples = read_pico_examples_pretraining(input_file=input_file, adversarial=args.adversarial)\n",
        "\n",
        "        elif args.ebmnet:\n",
        "            examples = read_pico_examples_ebmnet(input_file=input_file, adversarial=args.adversarial)\n",
        "\n",
        "        if args.pretraining:\n",
        "            features = convert_picos_to_features_pretraining(\n",
        "                examples=examples,\n",
        "                tokenizer=tokenizer,\n",
        "                max_pico_length=args.max_pico_length,\n",
        "                permutation=args.permutation\n",
        "            )\n",
        "        elif args.ebmnet:\n",
        "            features = convert_picos_to_features_ebmnet(\n",
        "                examples=examples,\n",
        "                tokenizer=tokenizer,\n",
        "                max_pico_length=args.max_pico_length,\n",
        "                permutation=args.permutation\n",
        "            )\n",
        "\n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        torch.save(features, cached_features_file)\n",
        "\n",
        "\n",
        "\t# Convert to Tensors and build dataset\n",
        "    all_ctx_ids = torch.tensor([f.ctx_id for f in features], dtype=torch.long)\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "\n",
        "    mlm2cls = {}\n",
        "    for i in range(34):\n",
        "        if i < 15:\n",
        "            mlm2cls[i] = 0\n",
        "        elif 15 <= i < 17:\n",
        "            mlm2cls[i] = 1\n",
        "        else:\n",
        "            mlm2cls[i] = 2\n",
        "\n",
        "    if args.num_labels == 3 and args.pretraining: # here we have 34 labels to be processed\n",
        "        all_labels = torch.tensor([mlm2cls[f.label] for f in features], dtype=torch.long)\n",
        "    else:\n",
        "        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "\n",
        "    all_example_ids = torch.tensor([f.example_index for f in features], dtype=torch.long)\n",
        "\n",
        "    dataset = TensorDataset(\n",
        "\t\tall_ctx_ids,\n",
        "\t\tall_input_ids,\n",
        "\t\tall_input_mask,\n",
        "\t\tall_segment_ids,\n",
        "\t\tall_labels,\n",
        "\t\tall_example_ids\n",
        "\t)\n",
        "\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "U9Se3VRcVgbR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertModel, BertConfig\n",
        "\n",
        "class EBM_Net(nn.Module):\n",
        "    def __init__(self, args, path=None):\n",
        "        super(EBM_Net, self).__init__()\n",
        "        self.args = args\n",
        "        self.config = BertConfig.from_pretrained(args.model_name_or_path)\n",
        "        self.bert = BertModel.from_pretrained(args.model_name_or_path)\n",
        "\n",
        "        num_cls = 34\n",
        "        self.res_linear = nn.Linear(self.config.hidden_size, num_cls)\n",
        "        if args.num_labels == 3:\n",
        "            self.final_linear = nn.Linear(num_cls, args.num_labels)\n",
        "        else:\n",
        "            self.final_linear = nn.Linear(self.config.hidden_size, args.num_labels)  # New linear layer\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.m = nn.LogSoftmax(dim=1)\n",
        "        self.loss = nn.NLLLoss()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        if path is not None:\n",
        "            pretrained_dict = torch.load(os.path.join(path, 'pytorch_model.bin'))\n",
        "            model_dict = self.state_dict()\n",
        "            to_load = {k: v for k, v in pretrained_dict.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
        "            print('HERE', model_dict.keys())\n",
        "\n",
        "            if len(to_load) != len(model_dict):\n",
        "                # Initialize new linear layer\n",
        "                nn.init.xavier_uniform_(self.final_linear.weight)\n",
        "                nn.init.zeros_(self.final_linear.bias)\n",
        "\n",
        "                # Add new layer to load state dict\n",
        "                model_dict['final_linear.weight'] = self.final_linear.weight\n",
        "                model_dict['final_linear.bias'] = self.final_linear.bias\n",
        "\n",
        "                model_dict.update(to_load)\n",
        "                self.load_state_dict(model_dict)\n",
        "\n",
        "    def forward(self, inputs, get_reprs=False):\n",
        "        cls_embeds = self.bert(input_ids=inputs['passage_ids'],\n",
        "\t\t\t\t\t\t\t   attention_mask=inputs['passage_mask'],\n",
        "\t\t\t\t\t\t\t   token_type_ids=inputs['passage_segment_ids'])[0][:, 0, :] # B x D\n",
        "\n",
        "        if get_reprs:\n",
        "            return cls_embeds\n",
        "\n",
        "        if self.args.num_labels == 3:\n",
        "            logits = self.final_linear(self.softmax(self.res_linear(cls_embeds))) # B x 3\n",
        "        else:\n",
        "            logits = self.res_linear(cls_embeds) # B x 34\n",
        "\n",
        "        if 'result_labels' in inputs:\n",
        "            return self.loss(self.m(logits), inputs['result_labels'])\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "    def save_pretrained(self, path):\n",
        "        # first save the model\n",
        "        torch.save(self.state_dict(), os.path.join(path, 'pytorch_model.bin'))\n",
        "        self.bert.save_pretrained(path)\n",
        "        # then save the config (vocab saved outside)\n",
        "        self.config.save_pretrained(path)\n"
      ],
      "metadata": {
        "id": "JiqbKjQcVqIX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import types\n",
        "import logging\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def main(model_name_or_path, output_dir, **kwargs):\n",
        "    args = types.SimpleNamespace()\n",
        "    args.model_name_or_path = model_name_or_path\n",
        "    args.output_dir = output_dir\n",
        "\n",
        "    args.train_ctx = kwargs.get(\"train_ctx\", None)\n",
        "    args.predict_ctx = kwargs.get(\"predict_ctx\", None)\n",
        "    args.repr_ctx = kwargs.get(\"repr_ctx\", None)\n",
        "    args.train_pico = kwargs.get(\"train_pico\", None)\n",
        "    args.predict_pico = kwargs.get(\"predict_pico\", None)\n",
        "    args.repr_pico = kwargs.get(\"repr_pico\", None)\n",
        "    args.permutation = kwargs.get(\"permutation\", \"ioc\")\n",
        "    args.tokenizer_name = kwargs.get(\"tokenizer_name\", \"\")\n",
        "    args.cache_dir = kwargs.get(\"cache_dir\", \"\")\n",
        "    args.max_passage_length = kwargs.get(\"max_passage_length\", 256)\n",
        "    args.max_pico_length = kwargs.get(\"max_pico_length\", 128)\n",
        "    args.do_train = kwargs.get(\"do_train\", False)\n",
        "    args.do_eval = kwargs.get(\"do_eval\", False)\n",
        "    args.do_repr = kwargs.get(\"do_repr\", False)\n",
        "    args.evaluate_during_training = kwargs.get(\"evaluate_during_training\", False)\n",
        "    args.do_lower_case = kwargs.get(\"do_lower_case\", False)\n",
        "    args.per_gpu_train_batch_size = kwargs.get(\"per_gpu_train_batch_size\", 24)\n",
        "    args.per_gpu_eval_batch_size = kwargs.get(\"per_gpu_eval_batch_size\", 24)\n",
        "    args.learning_rate = kwargs.get(\"learning_rate\", 5e-5)\n",
        "    args.gradient_accumulation_steps = kwargs.get(\"gradient_accumulation_steps\", 1)\n",
        "    args.weight_decay = kwargs.get(\"weight_decay\", 0.0)\n",
        "    args.adam_epsilon = kwargs.get(\"adam_epsilon\", 1e-8)\n",
        "    args.max_grad_norm = kwargs.get(\"max_grad_norm\", 1.0)\n",
        "    args.num_train_epochs = kwargs.get(\"num_train_epochs\", 3)\n",
        "    args.max_steps = kwargs.get(\"max_steps\", -1)\n",
        "    args.warmup_steps = kwargs.get(\"warmup_steps\", 400)\n",
        "    args.logging_steps = kwargs.get(\"logging_steps\", 200)\n",
        "    args.save_steps = kwargs.get(\"save_steps\", 100)\n",
        "    args.eval_all_checkpoints = kwargs.get(\"eval_all_checkpoints\", False)\n",
        "    args.no_cuda = kwargs.get(\"no_cuda\", False)\n",
        "    args.overwrite_cache = kwargs.get(\"overwrite_cache\", False)\n",
        "    args.seed = kwargs.get(\"seed\", 42)\n",
        "    args.local_rank = kwargs.get(\"local_rank\", -1)\n",
        "    args.pretraining = kwargs.get(\"pretraining\", False)\n",
        "    args.num_labels = kwargs.get(\"num_labels\", 3)\n",
        "    args.adversarial = kwargs.get(\"adversarial\", False)\n",
        "\n",
        "    args.overwrite_output_dir = True\n",
        "    if (\n",
        "        os.path.exists(args.output_dir)\n",
        "        and os.listdir(args.output_dir)\n",
        "        and args.do_train\n",
        "        and not args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
        "                args.output_dir\n",
        "            )\n",
        "        )\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "    args.device = device\n",
        "\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
        "    )\n",
        "    logger.warning(\n",
        "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s\",\n",
        "        args.local_rank,\n",
        "        device,\n",
        "        args.n_gpu,\n",
        "        bool(args.local_rank != -1)\n",
        "    )\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        do_lower_case=args.do_lower_case\n",
        "    )\n",
        "\n",
        "    model = EBM_Net(args, path=args.model_name_or_path)\n",
        "    model.to(args.device)\n",
        "\n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    if args.do_train:\n",
        "        train_ctxs = load_and_cache_ctxs(args, tokenizer, evaluate=False, pretraining=args.pretraining)\n",
        "        train_picos = load_and_cache_picos(args, tokenizer, evaluate=False, pretraining=args.pretraining)\n",
        "        global_step, tr_loss = train(args, train_picos, train_ctxs, model, tokenizer)\n",
        "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "        model.save_pretrained(args.output_dir)\n",
        "        tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
        "\n",
        "    if args.do_eval:\n",
        "        eval_ctxs = load_and_cache_ctxs(args, tokenizer, evaluate=True)\n",
        "        eval_picos = load_and_cache_picos(args, tokenizer, evaluate=True)\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        if args.do_train:\n",
        "            checkpoints = [args.output_dir]\n",
        "        else:\n",
        "            checkpoints = [args.model_name_or_path]\n",
        "\n",
        "        if args.eval_all_checkpoints:\n",
        "            checkpoints = list(\n",
        "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + 'pytorch_model.bin', recursive=True))\n",
        "            )\n",
        "\n",
        "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)\n",
        "\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "\n",
        "        for checkpoint in checkpoints:\n",
        "            if 'checkpoint' not in checkpoint:\n",
        "                global_step = 'final'\n",
        "            else:\n",
        "                global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
        "\n",
        "            model = EBM_Net(args, path=checkpoint)\n",
        "            model.to(args.device)\n",
        "\n",
        "            result = evaluate(args, eval_picos, eval_ctxs, model, tokenizer, prefix=global_step)\n",
        "\n",
        "            result = dict((k + (\"_{}\".format(global_step) if global_step else \"\"), v) for k, v in result.items())\n",
        "            results.update(result)\n",
        "\n",
        "            if 'checkpoint' in checkpoint and args.do_train and args.eval_all_checkpoints:\n",
        "                os.remove(os.path.join(checkpoint, 'full_model.bin'))\n",
        "                os.remove(os.path.join(checkpoint, 'pytorch_model.bin'))\n",
        "\n",
        "        logger.info(\"Results: {}\".format(results))\n",
        "        with open(os.path.join(args.output_dir, 'results.json'), 'w') as f:\n",
        "            results = {k: float(v) for k, v in results.items()}\n",
        "            json.dump(results, f, indent=4)\n",
        "\n",
        "    if args.do_repr:\n",
        "        logger.info(\"Representing...\")\n",
        "\n",
        "        model = EBM_Net(args, path=args.model_name_or_path)\n",
        "        model.to(args.device)\n",
        "\n",
        "        represent(args, model, tokenizer)"
      ],
      "metadata": {
        "id": "6mltLLT9V5Ab"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretraining"
      ],
      "metadata": {
        "id": "oIgYuky5Wa6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path  = f'{local_path}/biobert-v1.1'\n",
        "output_dir          = f'{local_path}/pretrained_model'\n",
        "do_train            = True\n",
        "train_pico          = f'{pretraining_dataset_path}/indexed_evidence.json'\n",
        "train_ctx           = f'{pretraining_dataset_path}/indexed_contexts.json'\n",
        "num_labels          = 34\n",
        "pretraining         = True\n",
        "adversarial         = True\n",
        "\n",
        "main(\n",
        "    model_name_or_path  = model_name_or_path,\n",
        "    output_dir          = output_dir,\n",
        "    do_train            = do_train,\n",
        "    train_pico          = train_pico,\n",
        "    train_ctx           = train_ctx,\n",
        "    num_labels          = num_labels,\n",
        "    pretraining         = pretraining,\n",
        "    adversarial         = adversarial\n",
        ")"
      ],
      "metadata": {
        "id": "FWL23dqgV_f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeb86453-60f4-4652-e130-84c1e3146228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetuning"
      ],
      "metadata": {
        "id": "2IFujf-UWTl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path  = f'{local_path}/pretrained_model'\n",
        "do_train            = True\n",
        "train_pico          = f'{evidence_integration_path}/indexed_train_picos.json'\n",
        "train_ctx           = f'{evidence_integration_path}/indexed_train_ctxs.json'\n",
        "do_eval             = True\n",
        "predict_pico        = f'{evidence_integration_path}/indexed_validation_picos.json'\n",
        "predict_ctx         = f'{evidence_integration_path}/indexed_validation_ctxs.json'\n",
        "output_dir          = f'{local_path}/drug_ebmnet_model'\n",
        "\n",
        "main(\n",
        "    model_name_or_path  = model_name_or_path,\n",
        "    do_train            = do_train,\n",
        "    train_pico          = train_pico,\n",
        "    train_ctx           = train_ctx,\n",
        "    do_eval             = do_eval,\n",
        "    predict_pico        = predict_pico,\n",
        "    predict_ctx         = predict_ctx,\n",
        "    output_dir          = output_dir\n",
        ")"
      ],
      "metadata": {
        "id": "fWNwqvZcWUbq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}